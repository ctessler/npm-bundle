\documentclass[a4paper,UKenglish,cleveref,autoref,english]{lipics-v2019}
%This is a template for producing LIPIcs articles. 
%See lipics-manual.pdf for further information.
%for A4 paper format use option "a4paper", for US-letter use option
%"letterpaper" 
%for british hyphenation rules use option "UKenglish", for american
%hyphenation rules use option "USenglish" 
%for section-numbered lemmas etc., use "numberwithinsect"
%for enabling cleveref support, use "cleveref"
%for enabling cleveref support, use "autoref"

%helpful if your graphic files are in another directory
\graphicspath{
  {../svg/}
  {./plot/UFS-alg/}
  {./plot/2D-UFS/}
  {./plot/avg-alg-sched/}
  {../aux}
  {./aux}
}

\bibliographystyle{plainurl}% the mandatory bibstyle

\title{\texttt{NPM-BUNDLE}: Non-Preemptive Multitask Scheduling for
  Jobs with \texttt{BUNDLE}-based Thread-Level Scheduling}

%optional, please use if title is longer than one line
\titlerunning{\texttt{NPM-BUNDLE}: Non-Preemptive Multitask \texttt{BUNDLE}}

\author{Corey Tessler}
       {Wayne State University, Detroit, Michigan, United States}
       {corey.tessler@wayne.edu}
       {}
       {}

\author{Nathan Fisher}
       {Wayne State University, Detroit, Michigan, United States}
       {fishern@wayne.edu}
       {}
       {}

%TODO mandatory. First: Use abbreviated first/middle names. Second
%(only in severe cases): Use first author plus 'et al.'        
\authorrunning{C. Tessler and N. Fisher}

%TODO mandatory, please use full first names. LIPIcs license is
%"CC-BY";  http://creativecommons.org/licenses/by/3.0/ 
\Copyright{Corey Tessler and Nathan Fisher}

%TODO mandatory: Please choose ACM 2012 classifications from
%https://dl.acm.org/ccs/ccs_flat.cfm
\ccsdesc{Computer systems organization~Real-time systems}
\ccsdesc{Software and its engineering~Real-time schedulability}

%TODO mandatory; please add comma-separated list of keywords
\keywords{Scheduling algorithms, Cache Memory, Multi-threading,
  Static Analysis}

%optional, e.g. invited paper
\category{}

%optional, e.g. full version hosted on arXiv, HAL, or other
%respository/website \relatedversion{A full version of the paper is
%available at %\url{...}.} 
\relatedversion{}

%optional, e.g. related research data, source code, ... hosted on a
%repository like zenodo, figshare, GitHub, ...
\supplement{}

%\funding{(Optional) general funding statement \dots}%optional, to
%capture a funding statement, which applies to all authors. Please
%enter author specific funding statements as fifth argument of the
%\author macro.
\funding{The research presented in this paper was supported
  by the National Science Foundation under Grant Nos. CNS-1618185 and
  IIS-1724227.}

\nolinenumbers %uncomment to disable line numbering

%\hideLIPIcs  %uncomment to remove references to LIPIcs series (logo,
%DOI, ...), e.g. when preparing a pre-final version to be uploaded to
%arXiv or another public repository 

%Editor-only macros:: begin (do not touch as%author) %%
\EventEditors{Sophie Quinton}
\EventNoEds{1}
\EventLongTitle{31st Euromicro Conference on Real-Time Systems (ECRTS 2019)}
\EventShortTitle{ECRTS 2019}
\EventAcronym{ECRTS}
\EventYear{2019}
\EventDate{July 9--12, 2019}
\EventLocation{Stuttgart, Germany}
\EventLogo{}
\SeriesVolume{133}
\ArticleNo{19}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Packages not supplied by LIPICS
%
% There are code listings, but no algorithm or pseudocode package 
\usepackage{algorithm,setspace}
\usepackage{algpseudocode}
\usepackage{relsize}
\usepackage{amsmath}
\usepackage{wrapfig} % Flowing text around figures.
\usepackage{subcaption} % subfigure is deprecated
\usepackage{gnuplottex}
% \usepackage{multirow} % For entries spanning multiple rows in tables
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newtheorem{prop}{Property}
\DeclareMathOperator{\argmax}{argmax}

\addtolength{\parskip}{-0.5mm}
\setlength{\textfloatsep}{0.1cm}
\setlength{\intextsep}{2pt}
\captionsetup{skip=4pt}
\setlength{\belowcaptionskip}{1pt}

\newcommand{\bundlep}{\texttt{BUNDLEP}}
\newcommand{\bundle}{\texttt{BUNDLE}}

\begin{document}
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Macros and Notation
% Macros for commonly used symbols and functions to ensure that their
% use remains consistent.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Block Reload Time - \BRT
\newcommand{\BRT}{\ensuremath{\mathbb{B}}}
% Cycles Per Instruction - \CPI
\newcommand{\CPI}{\ensuremath{\mathbb{I}}}
% Task Set - \tasks
\newcommand{\tasks}{\ensuremath{\tau}}
% Individual Task - \task{n}
\newcommand{\task}[1]{\ensuremath{\tau_{#1}}}
% Minimum inter-arrival time - \period{n}
\newcommand{\period}[1]{\ensuremath{p_{#1}}}
% Relative deadline - \deadline{n}
\newcommand{\deadline}[1]{\ensuremath{d_{#1}}}
% Absolute deadline - \Deadline{n}
\newcommand{\Deadline}[1]{\ensuremath{D_{#1}}}
% Number of threads released with each job of task n - \taskthreads{n}
\newcommand{\taskthreads}[1]{\ensuremath{m_{#1}}}
% WCET - \wcet{n}{m}
%    n - task number
%    m - number of threads
\newcommand{\wcet}[2]{\ensuremath{c_{#1}{(#2)}}}
% Object of execution - \object{i}
\newcommand{\object}[1]{\ensuremath{o_{#1}}}
% SLACK - \slack{${D_i}$}
\newcommand{\slack}[1]{\text{\sc{slack}(#1)}}
% DBF - \dbf{\tau_i}{\Deadline{k}}
\newcommand{\dbf}[2]{\text{\sc{dbf}(#1,#2)}}
% Hyper Period - \hp
\newcommand{\hp}{\ensuremath{P}}

% Superior indicator  - \supp{\tasks{}}
\newcommand{\supi}[1]{\ensuremath{\hat{#1}}}
% Superior Task Set - \supset
\newcommand{\supts}{\ensuremath{\hat{\tau}}}
% Anterior Task Set - \ants
\newcommand{\ants}{\ensuremath{\hat{\tau}}}
% Anterior task - \ant{i}
\newcommand{\ant}[1]{\ensuremath{\hat{\task{}}_#1}}

% Maximum Period and Relative Deadline Difference -
\newcommand{\pd}{\ensuremath{\Delta_{max}}}
% DIVIDE procedure \algdivide{t}{q}
%    t - the task
%    q - non preemptive chunk size
%    \algdivide{\task{i}}{q}
\newcommand{\algdivide}[2]{\text{\sc{divide}(#1,#2)}}
% DIVIDE as a name
\newcommand{\texdivide}{\text{\sc{divide}}}
% Partial tasks
%    i - the task being divided
%
\newcommand{\Partial}[1]{\ensuremath{\Phi_{#1}}}

% Growth Factor
\newcommand{\GFactor}{\ensuremath{\mathbb{F}}}

% NP-CHUNKS
\newcommand{\npchunks}{\text{\sc{np-chunks}}}
% BNG
\newcommand{\bng}{\text{\sc{bnc}}}
% TPJ
\newcommand{\tpj}{\text{\sc{tpj}}}

% S values
\newcommand{\stpj}{\ensuremath{s^{\text{\sc{tpj}}}}}

%
% To be overridden by over-one.tex
% 
\newcommand{\totalTaskSets}{-}
% Total number of task sets when \tau^1 utilization > 1
\newcommand{\totalOverOne}{-}
% Total number of OverOne task sets TPJ could schedule
\newcommand{\totalOOTPJ}{-}
% OverOne Ratio
\newcommand{\ratioOverOne}{-}
% TPJ ratio
\newcommand{\ratioOOTPJ}{-}
%
% end override
%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Abstract
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}

  The \bundle{} and \bundlep{} scheduling algorithms are cache-cognizant
  thread-level scheduling algorithms and associated worst case
  execution time and cache overhead (WCETO) techniques for hard
  real-time multi-threaded tasks. The \texttt{BUNDLE}-based approaches
  utilize the inter-thread cache benefit to reduce WCETO values for 
  jobs. Currently, the \texttt{BUNDLE}-based approaches are limited to
  scheduling a single task. This work aims to expand the applicability
  of \texttt{BUNDLE}-based scheduling to multiple task multi-threaded
  task sets.

  \texttt{BUNDLE}-based scheduling leverages knowledge of potential
  cache conflicts to selectively preempt one thread in favor of
  another from the same job. This thread-level preemption is a
  requirement for the run-time behavior and WCETO calculation to
  receive the benefit of \texttt{BUNDLE}-based approaches. This work
  proposes scheduling \texttt{BUNDLE}-based jobs non-preemptively
  according to the earliest deadline first (EDF) policy. Jobs are
  forbidden from preempting one another, while threads within a job
  are allowed to preempt other threads.

  An accompanying schedulability test is provided, named Threads Per
  Job (\tpj{}). \tpj{} is a novel schedulability test, input is a task
  set specification which may be transformed (under certain
  restrictions); dividing threads among tasks in an effort to find
  a feasible task set. Enhanced by the flexibility to transform task
  sets and taking advantage of the inter-thread cache benefit, the
  evaluation shows \tpj{} scheduling task sets fully preemptive EDF
  cannot. 
  
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Introduction
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

Hard real-time multi-threaded task systems which incorporate cache
memory, must account for the variation in execution time and cache
related preemption delays found in single-threaded task systems. For
multi-threaded task systems, the complexity of cache interactions is
increased due to thread-level cache interference and preemptions.
Worst-case execution time (WCET) and schedulability analysis of hard
real-time multi-threaded tasks commonly treat threads
independently~\cite{Pellizzoni:2011} or utilize cache management
techniques~\cite{Ward:2013} to limit the cache interference. 

Analysis techniques focusing on independent treatment or
limiting of cache interference exclude the possible benefit of
caches. Multi-threaded tasks may benefit from caches. By virtue of
sharing the same address space one thread of a task may cache values
on behalf of another reducing the total execution time to complete
both. This positive effect is referred to as the
\emph{inter-thread cache benefit}~\cite{Tessler:2016}. 

Currently, only the \bundle{}~\cite{Tessler:2016} and
\bundlep{}~\cite{Tessler:2018} analysis techniques and cache
congnizant thread-level scheduling algorithms incorporate the
inter-thread cache benefit into WCET and schedulability
analysis. These \texttt{BUNDLE}-based approaches are currently limited
to a \underline{single} multi-threaded task. The primary focus of this
work is to provide a scheduling algorithm and schedulability test for
multi-threaded task sets with multiple tasks, where individual jobs
utilize \texttt{BUNDLE}-based scheduling. As the first scheduling
algorithm to incorporate \texttt{BUNDLE}-based thread-level
scheduling, a non-preemptive algorithm was chosen to avoid necessary
modifications to \bundle{} and \bundlep{}. Non-preemptive EDF was
selected as the task-level scheduler, as the proposed schedulability
test presented in Section~\ref{sec:schedulability} is based upon
Baruah's limited-preemption for EDF~\cite{Baruah:2005} algorithm. 

An additional consideration is made for alternative approaches and
the unforeseen benefits to schedulability of thread-level schedulers
of non-preemptive multi-threaded jobs. If the WCET of jobs can be
expressed as a strictly increasing discrete concave function of the
number of threads per job, the schedulability test developed for this
work applies without modification to the \texttt{BUNDLE}-based
approaches or non-preemptive EDF scheduling. 

In the following sections, the key contributions are:
\begin{enumerate}
  \item A model of hard real-time multi-threaded tasks which is
    compatible with existing single-threaded models, where tasks sets
    may be transformed through division of threads.
  \item A schedulability test named Threads Per Job (\tpj{}) that provides a
    schedulability result and transformed feasible task set if the
    specified task set could not be scheduled non-preemptively.
  \item Proof of \tpj{}'s optimality with respect to non-preemptive
    multi-threaded feasibility.
  \item An improvement to Baruah's~\cite{Baruah:2005}
    non-preemptive chunk algorithm, increasing chunk sizes.
  \item An evaluation of over 500,000 task sets, comparing the
    schedulability ratio of \tpj{} to those of non-preemptive and
    (limited) preemptive EDF, with an accompanying implementation available for
    download~\cite{NPM-Artifact:2019}.
\end{enumerate}

These contributions are presented following the related
research of Section~\ref{sec:related}. Section~\ref{sec:model}
introduces the proposed model, application of non-preemptive EDF
scheduling for thread-level schedulers, and the requirements of task
transformation. Section~\ref{sec:schedulability} introduces then
improves upon the non-preemptive chunk algorithm~\cite{Baruah:2005},
followed by the \tpj{} schedulability algorithm and proof of
feasibility. Section~\ref{sec:eval} compares the schedulability ratio
of \tpj{} to other non-preemptive and preemptive scheduling
algorithms, before concluding with Section~\ref{sec:conclusion}. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Related Work
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion of Related Research}
\label{sec:related}

\noindent
{\bf Single-Threaded Tasks:}  The challenge of dealing with the
non-uniformity of execution times in real-time systems due to cache
misses or hits has received considerable attention~\cite{Wilhelm:2008,Theiling:2000}. In
particular, much of the 
prior real-time systems work on understanding caches vis-\`{a}-vis scheduling has focused upon the contention in the
cache due to tasks preempting each other.  Roughly speaking, a large
majority of this research can be classified into two categories:
\emph{cache-related preemption delay (CRPD) analysis} and
\emph{deferred/limited-preemption scheduling}. 
The goal of CRPD analysis is to bound the number of cache blocks of a
task that need to be reloaded due to evictions caused by a preempting
task.  The foundation of CRPD analysis is the development of
techniques for counting and bounding the number of blocks affected by
preemption; this is achieved by categorizing a task's cache blocks
into sets of useful cache blocks (UCBs) or evicting cache block
(ECBs)~\cite{Lee:1998,Tomiyama:2000}.  The size of these sets can be
used as an upper bound on the cache cost of a preemption.  Subsequent
research based upon this UCB and ECB categorization has refined these
sets and incorporated the CRPD analysis into schedulability
analysis~\cite{Altmeyer:2012,Altmeyer:2011,Altmeyer:2011b,Negi:2003,
  Staschulat:2005, Tan:2004}. 
However, please note that these CRPD approaches only quantify the
cache effect of preemption into existing scheduling approaches and do not
change any scheduling decision based upon the knowledge of
preemption. 

In limited/deferred-preemption scheduling, a higher-priority task may
preempt a lower-priority task only when some condition is satisfied.
The overall effect of deferring or limiting preemptions is to reduce
the number of times a task may be preempted during its execution.  The
hope is that by limiting the number of preemptions this will lead to a
decrease in the execution time of job due to the cache overhead of
preemption.  Different conditions for deferring preemptions have been
considered.  The fixed preemption-point approach~\cite{Burns:1995}
selects specific locations in a task code that are most appropriate
for the program but preserve the system schedulability.  The
preemption-threshold scheduling approach~\cite{Wang:1999} sets a
threshold that only task with higher-priority than this threshold may
preempt a currently-executing lower priority task.  The floating
preemption-point model~\cite{Baruah:2005,Marinho:2012} computes the
maximum time duration that a lower-priority task may delay the
preemption of a higher-priority task.  Each of the deferred preemption
approaches have been shown to limit the number of preemptions but do
not incorporate the CRPD overhead cost in its decision on how to defer
preemption. 

More recently, a line of research has emerged to combine the aspects
of CRPD analysis and limited/deferred preemption scheduling by
explicitly placing preemption points in the code to minimize CRPD
effects.  Early heuristics were proposed by Simonson and
Patel~\cite{Simonson:1995} and Lee et al.~\cite{Lee:1998}.  Bril et
al.~\cite{Bril:2017} integrated CRPD analysis into preemption-threshold
scheduling.  Bertogna et al.~\cite{Bertogna:2011} provide a more formal
approach for optimally determining preemptions in programs that can be
represented by linear control flowgraphs given the CRPD overhead of
each preemption and a bound on the maximum non-preemption
region~\cite{Baruah:2005}. Later work, extended this to more general
control flowgraphs~\cite{Peng:2014} or more precise CRPD
characterizations of the preemption costs~\cite{Cavicchio:2015}.
However, all of this aforementioned research assumes each task is
single-threaded.  The techniques proposed in this paper extend
the CRPD and limited preemption concepts to scheduling multi-threaded
tasks by combining and extending the limited-preemption scheduling
results of Baruah~\cite{Baruah:2005} to the cache-cognizant
thread-level scheduling algorithms that minimize cache contention
between threads called \bundle{}~\cite{Tessler:2016} and
\bundlep{}~\cite{Tessler:2018}.


\noindent
\textbf{Multi-Threaded Tasks:} Cache interference amplifies 
the variation in execution times of multi-threaded task sets. Threads
of the same task share cache locations, with the potential to increase
misses and hits depending on the order of execution of
threads. This variability is an addition to the variation already
present when considering CRPD with other tasks.

  There are few works we are aware which directly address the
inter-thread variability due to caches in multi-threaded task
sets. The approaches focus on isolating execution or managing cache
behavior. Memory-Centric Scheduling~\cite{Bak:2012} isolates
contentious execution by scheduling tasks according to their cache
behavior. To create such isolation, tasks must be
PREM-compliant~\cite{Pellizzoni:2011}, with distinct load and
execution phases. Cache management utilize techniques that limit the
contention in the cache, such as coloring and blocking found
in~\cite{Ward:2013}. These approaches come at a cost of modified or
restricted executable objects, reduced cache sizes, or additional
cache misses of blocked lines. Yet, with these limitations, the
inter-thread variability is not accounted for within multi-threaded
tasks. 

\bundle{}~\cite{Tessler:2016} and \bundlep{}~\cite{Tessler:2018}
address inter-thread variability due to cache interactions. These
\texttt{BUNDLE}-based approaches analyze executable object coupled
with a cache-cognizant thread-level scheduling algorithm without the
added detriment of modified (or restricted) objects, or cache
management penalties. We are not aware of any other technique that
addresses inter-thread variability, with the execption of
Calandrino's~\cite{Calandrino:2009} limited cache spread. However, the
results of~\cite{Calandrino:2009} are strictly empirical.



\input{20-model/00-model.tex}
\input{30-schedulability/00-schedulability.tex}
\input{70-evaluation/00-evaluation.tex}
\input{75-conclusion/00-conclusion.tex}

\bibliography{main}
\end{document}
